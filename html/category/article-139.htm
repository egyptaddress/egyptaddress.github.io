<!doctype html>
<html lang="zh-CN">

<head>
        <link rel="canonical" href="https://egyptaddress.github.io/html/category/article-139.htm" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <title>底层实现dropout——【torch学习笔记】 - EgyptAddress</title>
    <link rel="icon" href="/assets/addons/xcblog/img/egyptaddress/favicon.ico" type="image/x-icon"/>
        <link href="https://fonts.googleapis.com/css2?family=Jost:wght@300;400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/assets/addons/xcblog/css/egyptaddress/style-starter.css">
        </head>

<body>
        <!-- header -->
    <header id="site-header" class="fixed-top">
        <div class="container">
            <nav class="navbar navbar-expand-lg stroke">
                <a class="navbar-brand" href="/">
                                        <span class="fa fa-laptop"></span> EgyptAddress
                                    </a>
                <button class="navbar-toggler  collapsed bg-gradient" type="button" data-toggle="collapse" data-target="#navbarTogglerDemo02" aria-controls="navbarTogglerDemo02" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon fa icon-expand fa-bars"></span>
                    <span class="navbar-toggler-icon fa icon-close fa-times"></span>
                    </span>
                </button>
                <div class="collapse navbar-collapse" id="navbarTogglerDemo02">
                    <ul class="navbar-nav ml-auto">
                                                <li class="nav-item">
                            <a class="nav-link" href="/xcblog/">首页</a>
                        </li>
                                                <li class="nav-item">
                            <a class="nav-link" href="/category/">文章分类</a>
                        </li>
                                                <li class="nav-item">
                            <a class="nav-link" href="#">关于</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#">联系</a>
                        </li>
                    </ul>
                </div>
                <!-- toggle switch for light and dark theme -->
                <div class="mobile-position">
                    <nav class="navigation">
                        <div class="theme-switch-wrapper">
                            <label class="theme-switch" for="checkbox">
                                <input type="checkbox" id="checkbox">
                                <div class="mode-container">
                                    <i class="gg-sun"></i>
                                    <i class="gg-moon"></i>
                                </div>
                            </label>
                        </div>
                    </nav>
                </div>
                <!-- //toggle switch for light and dark theme -->
            </nav>
        </div>
    </header>
    <!-- //header -->
    
    <!-- about breadcrumb -->
    <section class="w3l-about-breadcrumb text-center">
        <div class="breadcrumb-bg breadcrumb-bg-about py-sm-5 py-4">
            <div class="container py-2">
                <h1 class="title" style="word-break: break-all;">底层实现dropout——【torch学习笔记】</h1>
                <ul class="breadcrumbs-custom-path mt-2">
                    <li><a href="/">首页</a></li>
                    <li><span class="fa fa-arrow-right mx-2" aria-hidden="true"></span></li>
                    <li><a href="/html/category/">文章分类</a></li>
                    <li class="active"><span class="fa fa-arrow-right mx-2" aria-hidden="true"></span> 正文</li>
                </ul>
            </div>
        </div>
    </section>
    <!-- //about breadcrumb -->
    <div class="container py-lg-5 py-3">
        <div class="row">
            <div class="col-md-8">
                <div class="post-content-content">
                      				  				  				<div id="content_views" class="markdown_views prism-github-gist"> <h1> <a id="dropout_0" rel="nofollow"></a>实现dropout</h1> <p>dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。</p> <p>引用翻译：《动手学深度学习》</p> <h2> <a id="_7" rel="nofollow"></a>一、重新审视过度拟合</h2> <p>鉴于特征比例子多得多，线性模型可以过度拟合。但是，当例子比特征多时，我们通常可以指望线性模型不会过度拟合。不幸的是，线性模型归纳的可靠性是有代价的。线性模型不能考虑到特征之间的相互作用。对于每个特征，线性模型必须分配一个正的或负的权重。它们缺乏考虑上下文的灵活性。</p> <p>在更正式的文本中，你会看到这种可概括性和灵活性之间的基本矛盾被讨论为偏倚-变异权衡。线性模型有很高的偏差（它们只能代表一小类函数），但方差很低（它们在数据的不同随机样本中给出相似的结果）。</p> <p>深度神经网络将我们带到了偏差-方差光谱的另一端。神经网络之所以如此灵活，是因为它们并不局限于单独观察每个特征。相反，它们可以学习各组特征之间的相互作用。例如，它们可以推断出 "尼日利亚 "和 "西联汇款 "同时出现在一封电子邮件中表明是垃圾邮件，但没有 "西联汇款 "的 "尼日利亚 "则不是。</p> <p>即使我们只有少量的特征，深度神经网络也有能力进行过度拟合。2017年，一组研究人员提出了一个现在众所周知的关于神经网络难以置信的灵活性的演示。他们向一个神经网络展示了随机标记的图像（没有真正的模式将输入和输出联系起来），并发现由SGD优化的神经网络可以完美地标记训练集中的每一张图像。</p> <p>考虑一下这意味着什么。如果标签是统一随机分配的，并且有10个类别，那么没有一个分类器能在保持数据上获得优于10%的准确性。然而，即使在这种情况下，当没有真正的模式可以学习时，神经网络也能完美地适应训练标签。</p> <h2> <a id="_19" rel="nofollow"></a>二、通过扰动的鲁棒性</h2> <p>让我们简单思考一下我们对一个好的统计模型的期望。我们希望它在未见过的测试数据上表现良好。我们可以通过询问什么是 "简单 "的模型来实现这一目标？简洁性可以以少量维度的形式出现，这就是我们在讨论用单项式基函数拟合模型时的做法。简洁性也可以以基函数的小规范的形式出现。这使我们想到了权重衰减（ℓ2正则化）。然而，我们可以施加的第三个简单性概念是，函数在输入的小变化下应该是稳健的。例如，当我们对图像进行分类时，我们希望在像素上添加一些随机噪声应该是无害的。</p> <p>1995年，Christopher Bishop正式提出了这个想法的一种形式，他证明了用输入噪声进行训练等同于Tikhonov正则化。换句话说，他在要求一个函数是平滑的（因而也是简单的）（我们在关于权重衰减的章节中讨论过）与要求它对输入的扰动有弹性之间建立了明确的数学联系。</p> <p>然后在2014年，Srivastava等人提出了一个聪明的想法，即如何将Bishop的想法也应用到网络的内部层。也就是说，他们提议在训练过程中，在计算后续层之前向网络的每一层注入噪声。他们意识到，在训练有很多层的深层网络时，仅仅在输入-输出映射上强制执行平滑性会忽略网络内部发生的事情。他们提出的想法被称为dropout，它现在是一种标准技术，被广泛用于训练神经网络。在整个训练过程中，在每一次迭代中，dropout正则化仅仅包括在计算下一层之前将每一层中的一些节点清零（通常是50%）。</p> <p>那么关键的挑战是如何在不引入不适当的统计偏差的情况下注入这种噪音。换句话说，我们希望在训练过程中对每一层的输入进行扰动，使该层的预期值等于我们没有引入任何噪声时的值。<br /> 在Bishop的例子中，当我们在线性模型中加入高斯噪声时，这很简单。在每个训练迭代中，只需向输入的????∼(0,????2)添加从均值为零的分布中采样的噪声，产生一个扰动点 ????′=????+????。在期望值中，????[????′]=????。</p> <p>在Dropout正则化的情况下，我们可以通过对未被Dropout的节点的比例进行归一化，来对每一层进行debias。换句话说，掉线概率为????的掉线被应用如下。</p> <p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"></p> <p>              h</p> <p>              ′</p> <p>             =</p> <p>              {</p> <p>                  0</p> <p>                   &nbsp;with&nbsp;probability&nbsp;</p> <p>                   p</p> <p>                   h</p> <p>                    1</p> <p>                    −</p> <p>                    p</p> <p>                  &nbsp;otherwise</p> <p>         \begin{aligned} h' = \begin{cases} 0 &amp; \text{ with probability } p \\ \frac{h}{1-p} &amp; \text{ otherwise} \end{cases} \end{aligned} </p> <p>     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 3.30003em; vertical-align: -1.40002em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.90002em;"><span class="" style="top: -3.90002em;"><span class="pstrut" style="height: 3.75em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;"><span class="delimsizing size4">{<!-- --></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.71455em;"><span class="" style="top: -3.71455em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord">0</span></span></span><span class="" style="top: -2.27455em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.880108em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">p</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.481108em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 1.21455em;"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.71455em;"><span class="" style="top: -3.71455em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">&nbsp;with&nbsp;probability&nbsp;</span></span><span class="mord mathdefault">p</span></span></span><span class="" style="top: -2.27455em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">&nbsp;otherwise</span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 1.21455em;"><span class=""></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 1.40002em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></p> <p>根据设计，期望值保持不变，即????[ℎ′]=ℎ。中间激活ℎ被一个具有匹配期望值的随机变量ℎ′所取代。Dropout "这一名称源于这样一个概念，即一些神经元为了计算最终结果而 “Dropout”。在训练过程中，我们用随机变量取代中间激活。</p> <h2> <a id="dropout_44" rel="nofollow"></a>三、实践中的dropout</h2> <p>回顾多层感知器（:numref:chapter_mlp），有一个隐藏层和5个隐藏单元。它的结构是这样的</p> <p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"></p> <p>            h</p> <p>             =</p> <p>             σ</p> <p>             (</p> <p>              W</p> <p>              1</p> <p>             x</p> <p>             +</p> <p>              b</p> <p>              1</p> <p>             )</p> <p>            o</p> <p>             =</p> <p>              W</p> <p>              2</p> <p>             h</p> <p>             +</p> <p>              b</p> <p>              2</p> <p>             y</p> <p>             ^</p> <p>             =</p> <p>              s</p> <p>              o</p> <p>              f</p> <p>              t</p> <p>              m</p> <p>              a</p> <p>              x</p> <p>             (</p> <p>             o</p> <p>             )</p> <p>         \begin{aligned} h &amp; = \sigma(W_1 x + b_1) \\ o &amp; = W_2 h + b_2 \\ \hat{y} &amp; = \mathrm{softmax}(o) \end{aligned} </p> <p>     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 4.5em; vertical-align: -2em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.5em;"><span class="" style="top: -4.66em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">h</span></span></span><span class="" style="top: -3.16em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">o</span></span></span><span class="" style="top: -1.66em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.19444em;">^</span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em;"><span class=""></span></span></span></span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 2em;"><span class=""></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.5em;"><span class="" style="top: -4.66em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span><span class="" style="top: -3.16em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -1.66em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord"><span class="mord mathrm">s</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right: 0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="mopen">(</span><span class="mord mathdefault">o</span><span class="mclose">)</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 2em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></p> <p>当我们对隐藏层应用dropout时，我们基本上是以概率????移除每个隐藏单元，（即把它们的输出设置为0）。我们可以把这个结果看作是一个只包含原始神经元子集的网络。在下面的图片中，ℎ2和ℎ5被删除。因此，????的计算不再依赖于ℎ2和ℎ5，它们各自的梯度也在执行Backprop时消失。这样一来，输出层的计算就不能过度依赖ℎ1,…,ℎ5中的任何一个元素。直观地说，深度学习的研究者们经常这样解释这个inutition：我们不希望网络的输出过于不稳定地依赖于通过网络的确切激活途径。dropout技术的原作者将他们的直觉描述为防止特征检测器的共同适应的一种努力。</p> <pre><code class="prism language-python"><span class="token keyword">from</span> IPython<span class="token punctuation">.</span>display <span class="token keyword">import</span> SVG SVG<span class="token punctuation">(</span>filename <span class="token operator">=</span> <span class="token string">'../img/dropout2.svg'</span><span class="token punctuation">)</span> </code></pre> <p><img decoding="async" src="http://img.555519.xyz/uploads/20230217/d211ae26a24966e1e22435a0bc2d3807.jpg" alt="底层实现dropout——【torch学习笔记】"></p> <p>在测试时，我们通常不使用辍学。然而，我们注意到有一些例外情况：一些研究人员在测试时使用辍学作为估计神经网络预测的信心的启发式评估：如果预测在许多不同的辍学掩码中都一致，那么我们可以说网络更有信心。现在，我们将把不确定性估计的高级话题推迟到以后的章节和卷中。</p> <h2> <a id="dropout_77" rel="nofollow"></a>四、从零开始实施dropout</h2> <p>为了实现单层的剔除功能，我们必须从伯努利（二进制）随机变量中抽取尽可能多的样本，因为我们的层有维数，其中随机变量的值为1（保留），概率为1-????，0（剔除），概率为????。一个简单的实现方法是首先从均匀分布????[0,1]中抽取样本。然后我们可以保留那些相应样本大于????的节点，放弃其余的节点。</p> <p>在下面的代码中，我们实现了一个dropout函数，该函数以drop_prob的概率丢掉了张量输入X中的元素，如上所述重新调整了剩余部分的比例（将幸存者除以1.0-drop_prob）。</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch <span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn <span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F <span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim  <span class="token keyword">import</span> sys sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'..'</span><span class="token punctuation">)</span> <span class="token keyword">def</span> <span class="token function">load_data_fashion_mnist</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> resize<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> root<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>         <span class="token string">'~'</span><span class="token punctuation">,</span> <span class="token string">'.pytorch'</span><span class="token punctuation">,</span> <span class="token string">'datasets'</span><span class="token punctuation">,</span> <span class="token string">'fashion-mnist'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token triple-quoted-string string">"""下载数据集并存入内存."""</span>     root <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>expanduser<span class="token punctuation">(</span>root<span class="token punctuation">)</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>root<span class="token punctuation">)</span>     transformer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>     <span class="token keyword">if</span> resize<span class="token punctuation">:</span>         transformer <span class="token operator">+=</span> <span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span>resize<span class="token punctuation">)</span><span class="token punctuation">]</span>     transformer <span class="token operator">+=</span> <span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>     transformer <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span>transformer<span class="token punctuation">)</span>          <span class="token comment"># 如果存在数据集则不再下载，若没有则下载</span>     mnist_train <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span>root<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transformer<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>     mnist_test <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span>root<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transformer<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>     num_workers <span class="token operator">=</span> <span class="token number">0</span> <span class="token keyword">if</span> sys<span class="token punctuation">.</span>platform<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">'win32'</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token number">4</span>          <span class="token comment"># 使用封装好的DataLoader载入即可。</span>     train_iter <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span>num_workers<span class="token punctuation">)</span>     test_iter <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>mnist_test<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span>num_workers<span class="token punctuation">)</span>     <span class="token keyword">return</span> mnist_train<span class="token punctuation">,</span>mnist_test<span class="token punctuation">,</span>train_iter<span class="token punctuation">,</span> test_iter   </code></pre> <p><strong>dropout的底层实现函数：</strong></p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">dropout</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> drop_prob<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token keyword">assert</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> drop_prob <span class="token operator">&lt;=</span> <span class="token number">1</span>     <span class="token keyword">if</span> drop_prob <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>  <span class="token comment"># 在这种情况下，所有元素都被剔除</span>         <span class="token keyword">return</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>X<span class="token punctuation">)</span>     <span class="token comment"># mask是计算是否保留的0，1值</span>     mask <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> drop_prob<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment"># 将幸存者除以1.0-drop_prob</span>     <span class="token keyword">return</span> mask <span class="token operator">*</span> X <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> drop_prob<span class="token punctuation">)</span> </code></pre> <p>我们可以在几个例子上测试一下dropout函数。在下面几行代码中，我们将我们的输入X通过dropout操作，概率分别为0、0.5和1。</p> <pre><code class="prism language-python"><span class="token comment"># mask = tensor([[0., 0., 0., 0., 0., 1., 1., 0.],</span> <span class="token comment">#        [0., 0., 1., 0., 0., 1., 0., 1.]])</span> X <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 相当于将幸存者除(1-0)（即不变）</span> <span class="token keyword">print</span><span class="token punctuation">(</span>dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 相当于将幸存者除(1-0.5)（即乘以2）</span> <span class="token keyword">print</span><span class="token punctuation">(</span>dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 相当于将幸存者除(1-1)（0）</span> </code></pre> <pre><code>tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],         [ 8.,  9., 10., 11., 12., 13., 14., 15.]]) tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],         [ 8.,  9., 10., 11., 12., 13., 14., 15.]]) tensor([[ 0.,  0.,  4.,  0.,  0., 10.,  0.,  0.],         [16., 18., 20.,  0.,  0.,  0.,  0., 30.]]) tensor([[0., 0., 0., 0., 0., 0., 0., 0.],         [0., 0., 0., 0., 0., 0., 0., 0.]]) </code></pre> <h2> <a id="_153" rel="nofollow"></a>五、定义模型参数</h2> <p>同样，我们可以使用 :numref:chapter_softmax_scratch中介绍的Fashion-MNIST数据集。我们将定义一个有两个隐藏层的多层感知器。这两个隐藏层都有256个输出。</p> <h2> <a id="dropout_157" rel="nofollow"></a>六、应用dropout定义模型</h2> <p>下面定义的模型将全连接层和激活函数ReLU串联起来，对每个激活函数的输出使用dropout。我们可以分别设置每一层的丢弃概率。一般来说，建议在靠近输入层的地方设置较低的丢弃概率。下面我们将第一和第二隐藏层分别设置为0.2和0.5。通过使用:numref:chapter_autograd中描述的is_training函数，我们可以确保dropout只在训练期间有效。</p> <pre><code class="prism language-python">drop_prob1<span class="token punctuation">,</span> drop_prob2 <span class="token operator">=</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span>  <span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_inputs <span class="token operator">=</span> <span class="token number">784</span><span class="token punctuation">,</span> num_outputs <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span> num_hiddens1 <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">,</span> num_hiddens2 <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">,</span> is_training <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                  self<span class="token punctuation">.</span>num_inputs <span class="token operator">=</span> num_inputs         self<span class="token punctuation">.</span>num_outputs <span class="token operator">=</span> num_outputs         self<span class="token punctuation">.</span>num_hiddens1 <span class="token operator">=</span> num_hiddens1         self<span class="token punctuation">.</span>num_hiddens2 <span class="token operator">=</span> num_hiddens2         self<span class="token punctuation">.</span>is_training <span class="token operator">=</span> is_training                  self<span class="token punctuation">.</span>linear_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">)</span>         self<span class="token punctuation">.</span>linear_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens1<span class="token punctuation">,</span> num_hiddens2<span class="token punctuation">)</span>         self<span class="token punctuation">.</span>linear_3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens2<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span>                  self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>          <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token comment"># 可以分别设置每一层的丢弃概率</span>         X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span>         H1 <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>         <span class="token comment"># 只有在训练阶段使用dropout</span>         <span class="token keyword">if</span> self<span class="token punctuation">.</span>is_training <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>             <span class="token comment"># 在第一个全连接层之后添加一个dropout层</span>             <span class="token comment"># 靠近输入层的地方设置较低的丢弃概率</span>             H1 <span class="token operator">=</span> dropout<span class="token punctuation">(</span>H1<span class="token punctuation">,</span> drop_prob1<span class="token punctuation">)</span>         H2 <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>H1<span class="token punctuation">)</span><span class="token punctuation">)</span>         <span class="token keyword">if</span> self<span class="token punctuation">.</span>is_training <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>             <span class="token comment"># 在第二个全连接层之后添加一个dropout层</span>             <span class="token comment"># 后面层的地方设置较中等的丢弃概率</span>             H2 <span class="token operator">=</span> dropout<span class="token punctuation">(</span>H2<span class="token punctuation">,</span> drop_prob2<span class="token punctuation">)</span>         out <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_3<span class="token punctuation">(</span>H2<span class="token punctuation">)</span>         <span class="token keyword">return</span> out  net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span> </code></pre> <pre><code>Net(   (linear_1): Linear(in_features=784, out_features=256, bias=True)   (linear_2): Linear(in_features=256, out_features=256, bias=True)   (linear_3): Linear(in_features=256, out_features=10, bias=True)   (relu): ReLU() ) </code></pre> <h2> <a id="_211" rel="nofollow"></a>七、训练和测试</h2> <p>这与前面描述的多层感知器的训练和测试类似。</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn <span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim <span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token keyword">import</span> math <span class="token keyword">import</span> time <span class="token keyword">def</span> <span class="token function">train_ch3</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token triple-quoted-string string">"""使用CPU训练或评估模型."""</span>     optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>     <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>         train_l_sum<span class="token punctuation">,</span> train_acc_sum<span class="token punctuation">,</span> n <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0</span>         <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>             <span class="token comment"># 即将梯度初始化为零</span>             optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>             <span class="token comment"># 通过训练的网络net处理该批次数据集X，得到预测的类别输出</span>             y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>             <span class="token comment"># 通过预测的类别 ，结合真实类别，通过交叉熵计算损失函数</span>             loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>             <span class="token comment"># loss.backward()函数的作用是根据loss来计算网络参数的梯度，其对应的输入默认为网络的叶子节点</span>             loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>             <span class="token comment"># 所有的optimizer都实现了step()方法，这个方法会更新所有的参数</span>             optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>             <span class="token comment"># 将真实类别转化为数值型</span>             y <span class="token operator">=</span> y<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>             <span class="token comment"># 计算损失的总和，pytorch中的.item()用于将一个零维张量转换成浮点数。可以减少gpu内存消耗</span>             train_l_sum <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>             <span class="token comment"># 如果真实label和预测的y_hat相同，则计数正确一个，以此计算训练集准确率</span>             train_acc_sum <span class="token operator">+=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>             n <span class="token operator">+=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>         <span class="token comment"># 通过评估函数计算测试集的准确率</span>         test_acc <span class="token operator">=</span> evaluate_accuracy<span class="token punctuation">(</span>test_iter<span class="token punctuation">,</span> net<span class="token punctuation">)</span>           <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span>\             <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> train_l_sum <span class="token operator">/</span> n<span class="token punctuation">,</span> train_acc_sum <span class="token operator">/</span> n<span class="token punctuation">,</span> test_acc<span class="token punctuation">)</span><span class="token punctuation">)</span> </code></pre> <pre><code class="prism language-python">num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">256</span> train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span> criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span> train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> lr<span class="token punctuation">)</span> </code></pre> <pre><code>epoch 1, loss 0.0036, train acc 0.659, test acc 0.770 epoch 2, loss 0.0021, train acc 0.803, test acc 0.801 epoch 3, loss 0.0018, train acc 0.831, test acc 0.781 epoch 4, loss 0.0017, train acc 0.845, test acc 0.837 epoch 5, loss 0.0016, train acc 0.851, test acc 0.812 epoch 6, loss 0.0015, train acc 0.860, test acc 0.846 epoch 7, loss 0.0014, train acc 0.864, test acc 0.843 epoch 8, loss 0.0014, train acc 0.869, test acc 0.851 epoch 9, loss 0.0014, train acc 0.873, test acc 0.861 epoch 10, loss 0.0013, train acc 0.875, test acc 0.853 </code></pre> <h2> <a id="_273" rel="nofollow"></a>八、摘要</h2> <p>1、除了控制维数和权重向量的大小之外，dropout是另一个避免过度拟合的工具。通常情况下，这三者是联合使用的。</p> <p>2、Dropout用随机变量ℎ′代替激活ℎ，其期望值为ℎ，方差由dropout概率????给出。<br /> 辍学只在训练期间使用。</p> <p>3、经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，因为这时候dropout随机生成的网络结构最多。</p> <p>4、dropout也可以被用作一种添加噪声的方法，直接对input进行操作，输入层设为更接近1的数，使得输入变化不会太大，一般选0.8。</p> <p>5、超参数的采样概率一般选1。</p> <p>6、dropout常用于提升模型泛化能力</p> <p>7、dropout也常用于解决模型费时的问题。做完dropout，相当于从原始的网络中找到一个更瘦的网络。因而，对于一个有N个节点的神经网络，有了dropout后，就可以看做是2^n个模型的集合了，但此时要训练的参数数目却是不变的，这就解决了费时的问题。</p> <h2> <a id="_292" rel="nofollow"></a>九、练习</h2> <p>1、试试如果你改变第1层和第2层的辍学概率会发生什么。特别是，如果你改变两层的概率会发生什么？</p> <p>2、增加历时的数量，比较使用放弃和不使用放弃时的结果。</p> <p>3、计算应用dropout后的激活随机变量的方差。</p> <p>4、为什么你通常不使用dropout？</p> <p>5、如果对模型进行修改，使其更加复杂，如增加隐藏层单元，使用dropout来应对过拟合的效果是否会更加明显？</p> <p>6、以本节中的模型为例，比较使用dropout和权重衰减的效果。如果同时使用dropout和权重衰减会怎样？</p> <p>7、如果我们对权重矩阵中的单个权重而不是激活应用dropout会怎样？</p> <p>8、用一个随机变量来代替dropout激活，该变量的值为[0,????/2,????] 。你能设计出比二进制dropout函数效果更好的东西吗？你为什么要使用它？为什么不呢？</p> </p></div> 			                </div>
                <div class="col-md-12 mt-5">
                                        <p>上一个：<a href="/html/category/article-138.htm">详解Android如何实现自定义的动画曲线_Android_</a></p>
                                        <p>下一个：<a href="/html/category/article-140.htm">Java中线程休眠的方法有几种？</a></p>
                                    </div>

                            </div>
            <div class="col-md-4 w3l-services">
                <h3 class="title-big mb-sm-3 mb-3">热门文章</h3>
<div class="row w3l-achievements">
    <div class="col-lg-12 item">
        <div class="card">
            <div class="box-wrap">
                <h4><a href="/html/category/article-216.htm">Spark连接Hive，进行Hive数据表的读写操作</a></h4>
                <p>基础环境   Hadoop安装-1，hadoop安装-2  spark安装 Hive安装   配置  将Hive的conf目录下的hive-site-xml文件拷贝到spark的conf目录下； 将H</p>
            </div>
        </div>
    </div>
</div>
<div class="row w3l-achievements">
    <div class="col-lg-12 item">
        <div class="card">
            <div class="box-wrap">
                <h4><a href="/html/category/article-246.htm">request和response——请求响应对象</a></h4>
                <p>请求对象——request 获取get请求的值  一次请求，返回一个响应。 地址栏：http://127.0.0.1:8000/day3/get_request?lan=python 问号：代表请求参</p>
            </div>
        </div>
    </div>
</div>
<div class="row w3l-achievements">
    <div class="col-lg-12 item">
        <div class="card">
            <div class="box-wrap">
                <h4><a href="/html/category/article-236.htm">「BUAA OO Unit 2 HW8」第二单元总结_在线工具</a></h4>
                <p>「BUAA OO Unit 2 HW8」第二单元总结   目录   「BUAA OO Unit 2 HW8」第二单元总结  Part 0 前言  Part 1 第五次作业  1.1 作业要求 1.2 </p>
            </div>
        </div>
    </div>
</div>
<div class="row w3l-achievements">
    <div class="col-lg-12 item">
        <div class="card">
            <div class="box-wrap">
                <h4><a href="/html/category/article-224.htm">Asp.Net在线预览Word文档的解决方案与思路</a></h4>
                <p>前几天有个老项目找到我，有多老呢？比我工作年限都长，见到这个项目我还得叫一声前辈。 这个项目目前使用非常稳定，十多年了没怎么更新过，现在客户想加一个小功能：在线预览Word文档。 首先想到的是用第三方</p>
            </div>
        </div>
    </div>
</div>
<div class="row w3l-achievements">
    <div class="col-lg-12 item">
        <div class="card">
            <div class="box-wrap">
                <h4><a href="/html/category/article-259.htm">C ++程序查找n个数的GCD和LCM</a></h4>
                <p>这是找出n个数字的GCD和LCM的代码。两个或更多不都是零的整数的GCD或最大公除数是将每个整数相除的最大正整数。GCD也被称为最大公因数。 两个数字的最小公倍数(LCM)是两个数字的倍数的最小数字（</p>
            </div>
        </div>
    </div>
</div>
<div class="row w3l-achievements">
    <div class="col-lg-12 item">
        <div class="card">
            <div class="box-wrap">
                <h4><a href="/html/category/article-254.htm">聊聊系统看门狗WDOG1在i.MXRT1xxx系统启动中的应用及影响</a></h4>
                <p>　　大家好，我是痞子衡，是正经搞技术的痞子。今天痞子衡给大家介绍的是系统看门狗WDOG1在i.MXRT1xxx系统启动中的应用及影响。 　　软件看门狗模块（WDOG）在 MCU 应用里可以说是非常基础</p>
            </div>
        </div>
    </div>
</div>
<div class="row w3l-achievements">
    <div class="col-lg-12 item">
        <div class="card">
            <div class="box-wrap">
                <h4><a href="/html/category/article-223.htm">C语言：关键字&#8212;union（声明共用体类型）</a></h4>
                <p>C语言32个关键字 有32个关键字详细说明，还有跳转链接！   一、union 简介 union 是C语言中一种声明共用体的数据类型。union(共用体)在某种程度上类似struct(结构体)的一种数</p>
            </div>
        </div>
    </div>
</div>
<div class="row w3l-achievements">
    <div class="col-lg-12 item">
        <div class="card">
            <div class="box-wrap">
                <h4><a href="/html/category/article-205.htm">SpringBoot扩展点EnvironmentPostProcessor_在线工具</a></h4>
                <p>一、背景 之前项目中用到了Apollo配置中心，对接Apollo配置中心后，配置中心的属性就可以在程序中使用了，那么这个是怎么实现的呢？配置中心的属性又是何时加载到程序中的呢？那么我们如果找到了这个是</p>
            </div>
        </div>
    </div>
</div>
<div class="row w3l-achievements">
    <div class="col-lg-12 item">
        <div class="card">
            <div class="box-wrap">
                <h4><a href="/html/category/article-211.htm">MaxCompute SQL函数详解 ODPS SQL函数详解之日期相关函数</a></h4>
                <p>MaxCompute SQL函数详解 ODPS SQL函数详解 日期函数  to_date函数  返回类型：datetime   语法：to_date(类型 参数1,类型 参数2); 	 to_dat</p>
            </div>
        </div>
    </div>
</div>
<div class="row w3l-achievements">
    <div class="col-lg-12 item">
        <div class="card">
            <div class="box-wrap">
                <h4><a href="/html/category/article-239.htm">ubuntu 18及以上版本配置IP的方法，你get了吗_在线工具</a></h4>
                <p>本文讲讲 Ubuntu 18 及以上版本配置 IP 的方法，为什么它值得一讲，因为以 Ubuntu 16 为首的版本的配置方法已经不适用了，如果你还不知道，那本文正好 get 一个新技能。 Ubunt</p>
            </div>
        </div>
    </div>
</div>

<h3 class="title-big mb-sm-3 mb-3">归纳</h3>
<div class="row w3l-achievements">
    <div class="col-lg-12 item">
        <div class="card">
            <div class="box-wrap">
                                <h4><span class="badge" style="float: right;">20</span> <a href="/date/2024-06/" title="2024-06 归档">2024-06</a></h4>
                                <h4><span class="badge" style="float: right;">62</span> <a href="/date/2024-05/" title="2024-05 归档">2024-05</a></h4>
                                <h4><span class="badge" style="float: right;">60</span> <a href="/date/2024-04/" title="2024-04 归档">2024-04</a></h4>
                                <h4><span class="badge" style="float: right;">62</span> <a href="/date/2024-03/" title="2024-03 归档">2024-03</a></h4>
                                <h4><span class="badge" style="float: right;">58</span> <a href="/date/2024-02/" title="2024-02 归档">2024-02</a></h4>
                            </div>
        </div>
    </div>
</div>
            </div>
        </div>
    </div>
    
        <!-- Footer -->
    <section class="w3l-footer py-sm-5 py-4">
        <div class="container">
            <div class="footer-content">
                <div class="row">
                    <div class="col-lg-8 footer-left">
                        <p class="m-0">EgyptAddress 版权所有</p>
                    </div>
                    <div class="col-lg-4 footer-right text-lg-right text-center mt-lg-0 mt-3">
                        <ul class="social m-0 p-0">
                            <li><a href="#facebook"><span class="fa fa-facebook-official"></span></a></li>
                            <li><a href="#linkedin"><span class="fa fa-linkedin-square"></span></a></li>
                            <li><a href="#instagram"><span class="fa fa-instagram"></span></a></li>
                            <li><a href="#twitter"><span class="fa fa-twitter"></span></a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <!-- move top -->
        <button onclick="topFunction()" id="movetop" title="Go to top">
            <span class="fa fa-angle-up"></span>
        </button>
        <script>
        // When the user scrolls down 20px from the top of the document, show the button
        window.onscroll = function() {
            scrollFunction()
        };

        function scrollFunction() {
            if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
                document.getElementById("movetop").style.display = "block";
            } else {
                document.getElementById("movetop").style.display = "none";
            }
        }

        // When the user clicks on the button, scroll to the top of the document
        function topFunction() {
            document.body.scrollTop = 0;
            document.documentElement.scrollTop = 0;
        }
        </script>
        <!-- /move top -->
    </section>
    <!-- //Footer -->

    <!-- all js scripts and files here -->
    <script src="/assets/addons/xcblog/js/frontend/egyptaddress/theme-change.js"></script><!-- theme switch js (light and dark)-->
    <script src="/assets/addons/xcblog/js/frontend/egyptaddress/jquery-3.3.1.min.js"></script><!-- default jQuery -->
    <!-- /typig-text-->
    <script>
    const typedTextSpan = document.querySelector(".typed-text");
    const cursorSpan = document.querySelector(".cursor");

    const textArray = ["UI/UX Designer", "Freelancer", "Web developer"];
    const typingDelay = 200;
    const erasingDelay = 10;
    const newTextDelay = 100; // Delay between current and next text
    let textArrayIndex = 0;
    let charIndex = 0;

    function type() {
        if (charIndex < textArray[textArrayIndex].length) {
            if (!cursorSpan.classList.contains("typing")) cursorSpan.classList.add("typing");
            typedTextSpan.textContent += textArray[textArrayIndex].charAt(charIndex);
            charIndex++;
            setTimeout(type, typingDelay);
        } else {
            cursorSpan.classList.remove("typing");
            setTimeout(erase, newTextDelay);
        }
    }

    function erase() {
        if (charIndex > 0) {
            // add class 'typing' if there's none
            if (!cursorSpan.classList.contains("typing")) {
                cursorSpan.classList.add("typing");
            }
            typedTextSpan.textContent = textArray[textArrayIndex].substring(0, 0);
            charIndex--;
            setTimeout(erase, erasingDelay);
        } else {
            cursorSpan.classList.remove("typing");
            textArrayIndex++;
            if (textArrayIndex >= textArray.length) textArrayIndex = 0;
            setTimeout(type, typingDelay);
        }
    }

    document.addEventListener("DOMContentLoaded", function() { // On DOM Load initiate the effect
        if (textArray.length) setTimeout(type, newTextDelay + 250);
    });
    </script>
    <!-- //typig-text-->
    <!-- services owlcarousel -->
    <script src="/assets/addons/xcblog/js/frontend/egyptaddress/owl.carousel.js"></script>
    <!-- script for services -->
    <script>
    $(document).ready(function() {
        $('.owl-two').owlCarousel({
            loop: true,
            margin: 30,
            nav: false,
            responsiveClass: true,
            autoplay: false,
            autoplayTimeout: 5000,
            autoplaySpeed: 1000,
            autoplayHoverPause: false,
            responsive: {
                0: {
                    items: 1,
                    nav: false
                },
                480: {
                    items: 1,
                    nav: false
                },
                700: {
                    items: 1,
                    nav: false
                },
                1090: {
                    items: 3,
                    nav: false
                }
            }
        })
    })
    </script>
    <!-- //script for services -->
    <!-- script for tesimonials carousel slider -->
    <script>
    $(document).ready(function() {
        $("#owl-demo1").owlCarousel({
            loop: true,
            margin: 20,
            nav: false,
            responsiveClass: true,
            responsive: {
                0: {
                    items: 1,
                    nav: false
                },
                736: {
                    items: 1,
                    nav: false
                },
                1000: {
                    items: 2,
                    nav: false,
                    loop: false
                }
            }
        })
    })
    </script>
    <!-- //script for tesimonials carousel slider -->
    <!-- video popup -->
    <script src="/assets/addons/xcblog/js/frontend/egyptaddress/jquery.magnific-popup.min.js"></script>
    <script>
    $(document).ready(function() {
        $('.popup-with-zoom-anim').magnificPopup({
            type: 'inline',

            fixedContentPos: false,
            fixedBgPos: true,

            overflowY: 'auto',

            closeBtnInside: true,
            preloader: false,

            midClick: true,
            removalDelay: 300,
            mainClass: 'my-mfp-zoom-in'
        });

        $('.popup-with-move-anim').magnificPopup({
            type: 'inline',

            fixedContentPos: false,
            fixedBgPos: true,

            overflowY: 'auto',

            closeBtnInside: true,
            preloader: false,

            midClick: true,
            removalDelay: 300,
            mainClass: 'my-mfp-slide-bottom'
        });
    });
    </script>
    <!-- //video popup -->
    <!-- stats number counter-->
    <script src="/assets/addons/xcblog/js/frontend/egyptaddress/jquery.waypoints.min.js"></script>
    <script src="/assets/addons/xcblog/js/frontend/egyptaddress/jquery.countup.js"></script>
    <script>
    $('.counter').countUp();
    </script>
    <!-- //stats number counter -->
    <!-- disable body scroll which navbar is in active -->
    <script>
    $(function() {
        $('.navbar-toggler').click(function() {
            $('body').toggleClass('noscroll');
        })

        $('.post-content-content .js_to').click(function(){
            var url = $(this).data('url');
            var code = $(this).data('code');
            url += code;

            window.open(url);
        })
    });
    </script>
    <!-- disable body scroll which navbar is in active -->
    <!--/MENU-JS-->
    <script>
    $(window).on("scroll", function() {
        var scroll = $(window).scrollTop();

        if (scroll >= 80) {
            $("#site-header").addClass("nav-fixed");
        } else {
            $("#site-header").removeClass("nav-fixed");
        }
    });

    //Main navigation Active Class Add Remove
    $(".navbar-toggler").on("click", function() {
        $("header").toggleClass("active");
    });
    $(document).on("ready", function() {
        if ($(window).width() > 991) {
            $("header").removeClass("active");
        }
        $(window).on("resize", function() {
            if ($(window).width() > 991) {
                $("header").removeClass("active");
            }
        });
    });
    </script>
    <!--//MENU-JS-->
    <!-- bootstrap js -->
    <script src="/assets/addons/xcblog/js/frontend/egyptaddress/bootstrap.min.js"></script>
</body>

</html>